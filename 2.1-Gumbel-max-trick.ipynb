{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cad924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch\t 2.0.1\n",
      "torch_scatter\t 2.1.1\n",
      "torch_geometric\t 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch_scatter\n",
    "from torch_scatter import scatter_log_softmax, scatter_max, scatter_sum\n",
    "\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.data as gd\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.utils import from_smiles\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"torch\\t\", torch.__version__)\n",
    "print(\"torch_scatter\\t\", torch_scatter.__version__)\n",
    "print(\"torch_geometric\\t\", torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d0d09",
   "metadata": {},
   "source": [
    "# Scatter Operation Using `torch_scatter`\n",
    "\n",
    "\n",
    "Unlike images, text and audio, graphs usually have irregular structures, which makes them hard to batch in tensor frameworks. Many existing implementations use padding to convert graphs into dense grid structures, which costs much unnecessary computation and memory.\n",
    "\n",
    "\n",
    "With `torch_scatter`, this notebook will show how we can deal with variadic inputs.\n",
    "\n",
    "\n",
    "See the figure below to see how `torch_scatter` works.\n",
    "\n",
    "\n",
    "<img width=\"50%\" src=\"https://raw.githubusercontent.com/rusty1s/pytorch_scatter/master/docs/source/_figures/add.svg?sanitize=true\" style=\"background-color:white;padding:20px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef3d522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 7, 5, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_scatter import scatter_sum\n",
    "\n",
    "index  = torch.LongTensor([0,0,1,0,2,2,3,3])\n",
    "input_ = torch.LongTensor([5,1,7,2,3,2,1,3])\n",
    "\n",
    "output = scatter_sum(input_, index)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ea7adc-a6f7-4234-b5e3-4fff231ddc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 7, 5, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_geometric.nn.global_add_pool gives the same functionality\n",
    "\n",
    "gnn.global_add_pool(input_, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918a91a-f67e-45ce-9abf-dfe4be9881c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bff8e6f",
   "metadata": {},
   "source": [
    "# Variable-sized Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7649d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([79])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class GCNPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = gnn.GCNConv(input_dim, emb_dim)\n",
    "        self.conv2 = gnn.GCNConv(emb_dim, emb_dim)\n",
    "        \n",
    "        self.glob_mlp = nn.Linear(emb_dim, 1)\n",
    "        self.node_mlp = nn.Linear(emb_dim, 1)\n",
    "        self.edge_mlp = nn.Linear(emb_dim, 1)\n",
    "        \n",
    "    def logits(self, g):\n",
    "        x, edge_index = g.x.float(), g.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        glob = gnn.global_add_pool(x, g.batch)\n",
    "        \n",
    "        i, j = edge_index\n",
    "        edge_feature = x[i] + x[j]\n",
    "        \n",
    "        glob_logits = self.glob_mlp(glob).flatten()\n",
    "        node_logits = self.node_mlp(x).flatten()\n",
    "        edge_logits = self.edge_mlp(edge_feature).flatten()\n",
    "        \n",
    "        logits = torch.cat([glob_logits, node_logits, edge_logits])\n",
    "        return logits\n",
    "\n",
    "\n",
    "d1 = from_smiles(\"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\") # caffeine\n",
    "d2 = from_smiles(\"CC(=O)NC1=CC=C(C=C1)O\") # acetaminophen\n",
    "\n",
    "data_list = [d1, d2]\n",
    "g = gd.Batch.from_data_list(data_list)\n",
    "g.num_edges = torch.LongTensor([d.num_edges for d in data_list])\n",
    "\n",
    "\n",
    "gcn = GCNPolicy(g.x.shape[1])\n",
    "logits = gcn.logits(g)\n",
    " \n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab075d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob_batch = torch.arange(g.num_graphs)\n",
    "node_batch = g.batch\n",
    "edge_batch = torch.repeat_interleave(g.num_edges)\n",
    "\n",
    "indices = torch.cat([glob_batch, node_batch, edge_batch])\n",
    "\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a3f924f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.1225, -6.6523, -3.7698, -3.8608, -3.8230, -3.8196, -3.8377, -3.8532,\n",
       "        -3.8855, -3.8294, -3.8896, -3.8974, -3.8367, -3.8742, -3.7648, -3.7648,\n",
       "        -3.5090, -3.6289, -3.5678, -3.5616, -3.5614, -3.5216, -3.5402, -3.6160,\n",
       "        -3.5402, -3.5216, -3.5569, -3.7128, -3.7128, -3.7558, -3.7766, -3.7558,\n",
       "        -3.7646, -3.7646, -3.7853, -3.7853, -3.7949, -3.7774, -3.7766, -3.7949,\n",
       "        -3.7953, -3.7953, -3.7666, -3.7776, -3.7666, -3.7776, -3.7694, -3.7126,\n",
       "        -3.7694, -3.7593, -3.7696, -3.7593, -3.7774, -3.7696, -3.7128, -3.7128,\n",
       "        -3.7126, -3.4040, -3.4040, -3.4231, -3.4430, -3.4231, -3.4430, -3.4777,\n",
       "        -3.4777, -3.4923, -3.4923, -3.4923, -3.4783, -3.4783, -3.5013, -3.5013,\n",
       "        -3.5013, -3.4821, -3.5013, -3.4783, -3.4923, -3.4783, -3.4821],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can calculate log probabilities for each logit using scatter_log_softmax. \n",
    "# But how can we get target probabilities that correspond to labels?\n",
    "\n",
    "y = torch.LongTensor([2, 13])\n",
    "scatter_log_softmax(logits, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c97adae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first sort the indices and logits\n",
    "sorted_indices, mapping = torch.sort(indices, stable=True)\n",
    "sorted_logits = logits[mapping]\n",
    "\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e46328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 58])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get target probabilities by adjusting indices\n",
    "count = torch.bincount(sorted_indices)\n",
    "offsets = torch.cumsum(count, 0) - count\n",
    "\n",
    "y + offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c69d50d-ed15-486d-bc8f-4aa78bb3760f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.8608, -3.4040], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = scatter_log_softmax(sorted_logits, sorted_indices)\n",
    "log_probs[y + offsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00dadc",
   "metadata": {},
   "source": [
    "# Gumbel-max trick\n",
    "\n",
    "Suppose we would like to obtain samples from multiple multinomial distributions, parameters given by variable-sized logits.\n",
    "\n",
    "We can sample without even calculating softmax using Gumbel-max trick.\n",
    "\n",
    "```\n",
    "u ~ uniform(len(logits))\n",
    "X ~ argmax(logits -log(-log(u)))\n",
    "```\n",
    "\n",
    "The noise added to logits, `-log(-log(u))`, follows Gumbel distribution, which gives the name.\n",
    "\n",
    "*Note*\n",
    "\n",
    "CDF of Gumbel distribution: \n",
    "\n",
    "$$ \n",
    "F(x) = \\exp\\{- \\exp\\{-x\\}\\}.\n",
    "$$\n",
    "\n",
    "Inverse CDF of Gumbel distribution: \n",
    "\n",
    "$$\n",
    "F^{-1}(y) = -\\log(-\\log y).\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Proof: https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bbdeb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can sample from logits using Gumbel-max trick\n",
    "\n",
    "logits = torch.FloatTensor([0, 1, 10, 2, 1, 0])\n",
    "indices = torch.LongTensor([1, 1,  1, 1, 0,  0])\n",
    "\n",
    "\n",
    "indices, mapping = torch.sort(indices, stable=True)\n",
    "logits = logits[mapping]\n",
    "\n",
    "unif = torch.rand_like(logits)\n",
    "gumbel = -(-unif.log()).log()\n",
    "_, max_indices = scatter_max(logits + gumbel, indices)\n",
    "\n",
    "count = torch.bincount(indices)\n",
    "offsets = torch.cumsum(count, 0) - count\n",
    "samples =  max_indices - offsets\n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80142d43-3e8c-43dd-b03d-3e8b8508287f",
   "metadata": {},
   "source": [
    "# Question\n",
    "    \n",
    "1. `torch.rand_like(logits)`와 `torch.rand(len(logits))`와의 차이는??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e5c7ba-469c-43cb-8c12-8d24a986e418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
