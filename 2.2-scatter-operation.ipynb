{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53cad924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch\t 2.0.1\n",
      "torch_scatter\t 2.1.1\n",
      "torch_geometric\t 2.3.1\n"
     ]
    }
   ],
   "source": [
    "import torch_scatter\n",
    "from torch_scatter import scatter_log_softmax, scatter_max\n",
    "\n",
    "\n",
    "import torch_geometric\n",
    "import torch_geometric.data as gd\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.utils import from_smiles\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"torch\\t\", torch.__version__)\n",
    "print(\"torch_scatter\\t\", torch_scatter.__version__)\n",
    "print(\"torch_geometric\\t\", torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d0d09",
   "metadata": {},
   "source": [
    "# Scatter Operation Using `torch_scatter`\n",
    "\n",
    "\n",
    "Unlike images, text and audio, graphs usually have irregular structures, which makes them hard to batch in tensor frameworks. Many existing implementations use padding to convert graphs into dense grid structures, which costs much unnecessary computation and memory.\n",
    "\n",
    "\n",
    "With `torch_scatter`, this notebook will show how we can deal with variadic inputs.\n",
    "\n",
    "\n",
    "See the figure below to see how `torch_scatter` works.\n",
    "\n",
    "\n",
    "<img width=\"50%\" src=\"https://raw.githubusercontent.com/rusty1s/pytorch_scatter/master/docs/source/_figures/add.svg?sanitize=true\" style=\"background-color:white;padding:20px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef3d522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 7, 5, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_scatter import scatter_sum\n",
    "\n",
    "index  = torch.LongTensor([0,0,1,0,2,2,3,3])\n",
    "input_ = torch.LongTensor([5,1,7,2,3,2,1,3])\n",
    "\n",
    "output = scatter_sum(input_, index)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da98b11",
   "metadata": {},
   "source": [
    "# torch.distributions.Categorical\n",
    "\n",
    "We want to use something similar to the `torch.distributions.Categorical`. \n",
    "\n",
    "But `torch.distributions.Categorical` can only take fixed sized tensors. \n",
    "\n",
    "Let's see how `Categorical` works first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ce8591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[25, 9], edge_index=[2, 52], edge_attr=[52, 3], smiles=[2], batch=[25], ptr=[3], num_edges=[2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use below `g` as an example data\n",
    "\n",
    "d1 = from_smiles(\"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\") # caffeine\n",
    "d2 = from_smiles(\"CC(=O)NC1=CC=C(C=C1)O\") # acetaminophen\n",
    "\n",
    "data_list = [d1, d2]\n",
    "g = gd.Batch.from_data_list(data_list)\n",
    "g.num_edges = torch.LongTensor([d.num_edges for d in data_list])\n",
    "\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9eb1ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorical(logits: torch.Size([2, 5]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCNPolicy1(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = gnn.GCNConv(input_dim, emb_dim)\n",
    "        self.conv2 = gnn.GCNConv(emb_dim, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "    def forward(self, g):\n",
    "        x, edge_index = g.x.float(), g.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        glob = gnn.global_add_pool(x, g.batch)\n",
    "        logits = self.linear(glob)\n",
    "        \n",
    "        # Suppose we are to decide what type of atoms to add in the input graph.\n",
    "        # Since the number of atom types are fixed, we can use `Categorical`.\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gcn = GCNPolicy1(\n",
    "    input_dim=g.x.shape[1], \n",
    "    output_dim=5\n",
    ")\n",
    "cat = gcn(g)\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812902b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can sample from the distribution given by logits\n",
    "cat.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c7433c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.9723, -4.1078], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get log probabilities corresponding to the target values.\n",
    "y = torch.LongTensor([1, 0])\n",
    "cat.log_prob(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff8e6f",
   "metadata": {},
   "source": [
    "# Variable-sized Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc7649d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([79])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GCNPolicy2(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = gnn.GCNConv(input_dim, emb_dim)\n",
    "        self.conv2 = gnn.GCNConv(emb_dim, emb_dim)\n",
    "        \n",
    "        self.glob_mlp = nn.Linear(emb_dim, 1)\n",
    "        self.node_mlp = nn.Linear(emb_dim, 1)\n",
    "        self.edge_mlp = nn.Linear(emb_dim, 1)\n",
    "        \n",
    "    def logits(self, g):\n",
    "        x, edge_index = g.x.float(), g.edge_index\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        glob = gnn.global_add_pool(x, g.batch)\n",
    "        \n",
    "        i, j = edge_index\n",
    "        edge_feature = x[i] + x[j]\n",
    "        \n",
    "        glob_logits = self.glob_mlp(glob).flatten()\n",
    "        node_logits = self.node_mlp(x).flatten()\n",
    "        edge_logits = self.edge_mlp(edge_feature).flatten()\n",
    "        \n",
    "        logits = torch.cat([glob_logits, node_logits, edge_logits])\n",
    "        return logits\n",
    "        \n",
    "gcn = GCNPolicy2(g.x.shape[1])\n",
    "logits = gcn.logits(g)\n",
    " \n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab075d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob_batch = torch.arange(g.num_graphs)\n",
    "node_batch = g.batch\n",
    "edge_batch = torch.arange(g.num_graphs).repeat_interleave(g.num_edges)\n",
    "\n",
    "indices = torch.cat([glob_batch, node_batch, edge_batch])\n",
    "\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a3f924f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.7812, -2.8636, -3.7907, -3.9286, -3.9379, -3.9802, -3.9975, -4.0006,\n",
       "        -4.0072, -3.9304, -3.9490, -3.9848, -3.9330, -3.9475, -3.7976, -3.7976,\n",
       "        -3.5355, -3.6587, -3.6289, -3.6437, -3.6923, -3.6548, -3.6645, -3.7142,\n",
       "        -3.6645, -3.6548, -3.6392, -3.6885, -3.6885, -3.7759, -3.8233, -3.7759,\n",
       "        -3.8174, -3.8174, -3.8584, -3.8584, -3.8744, -3.8373, -3.8233, -3.8744,\n",
       "        -3.8898, -3.8898, -3.8590, -3.8570, -3.8590, -3.8570, -3.8422, -3.7204,\n",
       "        -3.8422, -3.8459, -3.8379, -3.8459, -3.8373, -3.8379, -3.7160, -3.7160,\n",
       "        -3.7204, -3.4589, -3.4589, -3.5222, -3.5206, -3.5222, -3.5206, -3.5231,\n",
       "        -3.5231, -3.5156, -3.5156, -3.5156, -3.4954, -3.4954, -3.5286, -3.5286,\n",
       "        -3.5286, -3.5239, -3.5286, -3.4954, -3.5156, -3.4954, -3.5239],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can calculate log probabilities. \n",
    "# But how can we get target probabilities that correspond to y?\n",
    "\n",
    "y = torch.LongTensor([2, 13])\n",
    "scatter_log_softmax(logits, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c97adae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first sort the indices and logits\n",
    "sorted_indices, mapping = torch.sort(indices, stable=True)\n",
    "sorted_logits = logits[mapping]\n",
    "log_probs = scatter_log_softmax(sorted_logits, sorted_indices)\n",
    "\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e46328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.9286, -3.4589], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get target probabilities by adjusting indices\n",
    "count = torch.bincount(sorted_indices)\n",
    "offsets = torch.cumsum(count, 0) - count\n",
    "\n",
    "log_probs[y + offsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00dadc",
   "metadata": {},
   "source": [
    "# Gumbel-max trick\n",
    "\n",
    "`logits`에서 샘플링을 하고 싶을 때, softmax 값을 직접 구하지 않아도 되는 방법이 있다.\n",
    "\n",
    "아래 식과 같이 `X`를 샘플링을 하면, 이는 softmax에서 샘플링을 한 것과 같다.\n",
    "\n",
    "```\n",
    "u ~ uniform(len(uniform))\n",
    "X ~ argmax(-log(-log(u)))\n",
    "```\n",
    "\n",
    "이를 Gumbel-max trick이라 부르는데, 그 이유는 `-log(-log(u))`가 Gumbel 분포를 따르기 때문이다.\n",
    "\n",
    "*참고*\n",
    "\n",
    "    Gumbel 분포의 CDF는 다음과 같으며,\n",
    "\n",
    "$$\n",
    "F(x) = \\exp\\{- \\exp\\{-x\\}\\}.\n",
    "$$\n",
    "\n",
    "    CDF의 역함수는 다음과 같다\n",
    "\n",
    "$$\n",
    "F^{-1}(y) = -\\log(-\\log y).\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- Gumbel-max trick 증명\n",
    "\n",
    "    https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bbdeb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can sample from logits using Gumbel-max trick\n",
    "\n",
    "logits = torch.FloatTensor([0, 1, 10, 2, 1, 0])\n",
    "indices = torch.LongTensor([1, 1,  1, 1, 0,  0])\n",
    "\n",
    "\n",
    "indices, mapping = torch.sort(indices, stable=True)\n",
    "logits = logits[mapping]\n",
    "\n",
    "unif = torch.rand_like(logits)\n",
    "gumbel = -(-unif.log() ).log()\n",
    "_, max_indices = scatter_max(logits + gumbel, indices)\n",
    "\n",
    "count = torch.bincount(indices)\n",
    "offsets = torch.cumsum(count, 0) - count\n",
    "samples =  max_indices - offsets\n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80142d43-3e8c-43dd-b03d-3e8b8508287f",
   "metadata": {},
   "source": [
    "# Question\n",
    "    \n",
    "1. `torch.rand_like(logits)`와 `torch.rand(len(logits))`와의 차이는??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e5c7ba-469c-43cb-8c12-8d24a986e418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
